{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Fundamentals: From Theory to Practice\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to Reinforcement Learning](#intro)\n",
    "2. [Exploration vs Exploitation](#exploration)\n",
    "3. [Markov Decision Processes (MDPs)](#mdp)\n",
    "4. [Value Functions vs Policies](#value-policy)\n",
    "5. [Value Function Approximation](#approximation)\n",
    "6. [Deep Q-Network Implementation for MountainCar](#dqn)\n",
    "7. [Training Visualization and Analysis](#visualization)\n",
    "\n",
    "---\n",
    "\n",
    "This notebook provides a comprehensive introduction to Reinforcement Learning concepts with practical implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import all necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure ML Compatible Setup\n",
    "# First, let's fix the numpy compatibility issue\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Check current environment\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "\n",
    "# Fix numpy compatibility for Azure ML\n",
    "try:\n",
    "    # Uninstall and reinstall packages in correct order\n",
    "    print(\"Fixing numpy compatibility...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"--force-reinstall\", \"numpy==1.23.5\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"--force-reinstall\", \"pandas==1.5.3\"])\n",
    "    print(\"Numpy and pandas reinstalled successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not reinstall packages: {e}\")\n",
    "    print(\"Continuing with existing packages...\")\n",
    "\n",
    "# Now import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import seaborn, but continue without it if it fails\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    sns.set_palette(\"husl\")\n",
    "    SEABORN_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    print(f\"Warning: Seaborn not available ({e}). Using matplotlib only.\")\n",
    "    SEABORN_AVAILABLE = False\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# OpenAI Gym\n",
    "import gym\n",
    "\n",
    "# Visualization settings\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "except:\n",
    "    try:\n",
    "        plt.style.use('seaborn-darkgrid')\n",
    "    except:\n",
    "        plt.style.use('ggplot')\n",
    "        print(\"Using ggplot style instead of seaborn\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Gym version: {gym.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='intro'></a>1. Introduction to Reinforcement Learning\n",
    "\n",
    "Reinforcement Learning (RL) is a paradigm of machine learning where an **agent** learns to make decisions by interacting with an **environment**. The agent receives **rewards** or **penalties** based on its actions, and its goal is to learn a strategy (policy) that maximizes cumulative reward over time.\n",
    "\n",
    "### Key Components:\n",
    "- **Agent**: The learner and decision maker\n",
    "- **Environment**: Everything the agent interacts with\n",
    "- **State (s)**: The current situation of the agent\n",
    "- **Action (a)**: What the agent can do\n",
    "- **Reward (r)**: Feedback from the environment\n",
    "- **Policy (π)**: The agent's strategy for choosing actions\n",
    "- **Value Function (V)**: Expected future reward from a state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='exploration'></a>2. Exploration vs Exploitation\n",
    "\n",
    "One of the fundamental challenges in RL is the **exploration-exploitation dilemma**:\n",
    "- **Exploitation**: Choose the best-known action to maximize immediate reward\n",
    "- **Exploration**: Try new actions to discover potentially better long-term strategies\n",
    "\n",
    "Let's demonstrate this with a simple multi-armed bandit problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmedBandit:\n",
    "    \"\"\"Simple multi-armed bandit environment for demonstrating exploration vs exploitation.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms=10):\n",
    "        self.n_arms = n_arms\n",
    "        # True reward probabilities (unknown to agent)\n",
    "        self.true_probs = np.random.beta(2, 2, n_arms)\n",
    "        \n",
    "    def pull(self, arm):\n",
    "        \"\"\"Pull an arm and receive a reward.\"\"\"\n",
    "        return 1 if np.random.random() < self.true_probs[arm] else 0\n",
    "    \n",
    "    def get_optimal_arm(self):\n",
    "        \"\"\"Return the index of the best arm.\"\"\"\n",
    "        return np.argmax(self.true_probs)\n",
    "\n",
    "\n",
    "class EpsilonGreedyAgent:\n",
    "    \"\"\"Agent using epsilon-greedy strategy for exploration.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms, epsilon=0.1):\n",
    "        self.n_arms = n_arms\n",
    "        self.epsilon = epsilon\n",
    "        self.q_values = np.zeros(n_arms)  # Estimated values\n",
    "        self.n_pulls = np.zeros(n_arms)   # Number of times each arm was pulled\n",
    "        \n",
    "    def select_arm(self):\n",
    "        \"\"\"Select an arm using epsilon-greedy strategy.\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Exploration: choose random arm\n",
    "            return np.random.randint(self.n_arms)\n",
    "        else:\n",
    "            # Exploitation: choose best known arm\n",
    "            return np.argmax(self.q_values)\n",
    "    \n",
    "    def update(self, arm, reward):\n",
    "        \"\"\"Update Q-value estimates.\"\"\"\n",
    "        self.n_pulls[arm] += 1\n",
    "        # Incremental update rule\n",
    "        self.q_values[arm] += (reward - self.q_values[arm]) / self.n_pulls[arm]\n",
    "\n",
    "\n",
    "# Experiment: Compare different epsilon values\n",
    "def run_bandit_experiment(epsilon_values, n_steps=1000, n_runs=100):\n",
    "    results = {eps: [] for eps in epsilon_values}\n",
    "    \n",
    "    for eps in epsilon_values:\n",
    "        cumulative_regret = np.zeros(n_steps)\n",
    "        \n",
    "        for run in range(n_runs):\n",
    "            bandit = MultiArmedBandit()\n",
    "            agent = EpsilonGreedyAgent(bandit.n_arms, epsilon=eps)\n",
    "            optimal_arm = bandit.get_optimal_arm()\n",
    "            \n",
    "            regret = 0\n",
    "            for step in range(n_steps):\n",
    "                arm = agent.select_arm()\n",
    "                reward = bandit.pull(arm)\n",
    "                agent.update(arm, reward)\n",
    "                \n",
    "                # Calculate regret (difference from optimal)\n",
    "                optimal_reward = bandit.true_probs[optimal_arm]\n",
    "                actual_reward = bandit.true_probs[arm]\n",
    "                regret += optimal_reward - actual_reward\n",
    "                cumulative_regret[step] += regret\n",
    "        \n",
    "        results[eps] = cumulative_regret / n_runs\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Run experiment\n",
    "epsilon_values = [0.0, 0.01, 0.1, 0.3]\n",
    "results = run_bandit_experiment(epsilon_values)\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for eps, regret in results.items():\n",
    "    plt.plot(regret, label=f'ε = {eps}', linewidth=2)\n",
    "\n",
    "plt.xlabel('Steps', fontsize=12)\n",
    "plt.ylabel('Cumulative Regret', fontsize=12)\n",
    "plt.title('Exploration vs Exploitation: Effect of Epsilon on Cumulative Regret', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Analysis:\")\n",
    "print(\"- ε = 0.0 (pure exploitation): Gets stuck with suboptimal arms\")\n",
    "print(\"- ε = 0.01: Balances exploration and exploitation well\")\n",
    "print(\"- ε = 0.1: Good balance for this problem\")\n",
    "print(\"- ε = 0.3: Too much exploration, slower convergence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='mdp'></a>3. Markov Decision Processes (MDPs)\n",
    "\n",
    "An MDP is a mathematical framework for modeling decision-making problems. It consists of:\n",
    "- **S**: Set of states\n",
    "- **A**: Set of actions\n",
    "- **P**: State transition probability matrix P(s'|s,a)\n",
    "- **R**: Reward function R(s,a,s')\n",
    "- **γ**: Discount factor (0 ≤ γ ≤ 1)\n",
    "\n",
    "The **Markov property** states that the future depends only on the current state, not the history.\n",
    "\n",
    "Let's create a simple grid world MDP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldMDP:\n",
    "    \"\"\"Simple grid world environment to demonstrate MDP concepts.\"\"\"\n",
    "    \n",
    "    def __init__(self, size=5):\n",
    "        self.size = size\n",
    "        self.n_states = size * size\n",
    "        self.n_actions = 4  # Up, Down, Left, Right\n",
    "        \n",
    "        # Define special states\n",
    "        self.goal_state = (size-1, size-1)  # Bottom-right corner\n",
    "        self.trap_states = [(1, 1), (2, 3), (3, 1)]  # Trap states\n",
    "        \n",
    "        # Action mappings\n",
    "        self.actions = {\n",
    "            0: (-1, 0),  # Up\n",
    "            1: (1, 0),   # Down\n",
    "            2: (0, -1),  # Left\n",
    "            3: (0, 1)    # Right\n",
    "        }\n",
    "        \n",
    "        # Initialize transition probabilities and rewards\n",
    "        self._init_dynamics()\n",
    "    \n",
    "    def _init_dynamics(self):\n",
    "        \"\"\"Initialize transition probabilities and rewards.\"\"\"\n",
    "        self.transitions = {}\n",
    "        self.rewards = {}\n",
    "        \n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                state = (i, j)\n",
    "                \n",
    "                for action in range(self.n_actions):\n",
    "                    # Deterministic transitions (can be made stochastic)\n",
    "                    next_state = self._get_next_state(state, action)\n",
    "                    self.transitions[(state, action)] = next_state\n",
    "                    \n",
    "                    # Define rewards\n",
    "                    if next_state == self.goal_state:\n",
    "                        self.rewards[(state, action, next_state)] = 10.0\n",
    "                    elif next_state in self.trap_states:\n",
    "                        self.rewards[(state, action, next_state)] = -5.0\n",
    "                    else:\n",
    "                        self.rewards[(state, action, next_state)] = -0.1\n",
    "    \n",
    "    def _get_next_state(self, state, action):\n",
    "        \"\"\"Get next state given current state and action.\"\"\"\n",
    "        di, dj = self.actions[action]\n",
    "        next_i = max(0, min(self.size-1, state[0] + di))\n",
    "        next_j = max(0, min(self.size-1, state[1] + dj))\n",
    "        return (next_i, next_j)\n",
    "    \n",
    "    def step(self, state, action):\n",
    "        \"\"\"Take a step in the environment.\"\"\"\n",
    "        next_state = self.transitions[(state, action)]\n",
    "        reward = self.rewards[(state, action, next_state)]\n",
    "        done = (next_state == self.goal_state)\n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def render(self, state=None, values=None):\n",
    "        \"\"\"Visualize the grid world.\"\"\"\n",
    "        grid = np.zeros((self.size, self.size))\n",
    "        \n",
    "        # Mark special states\n",
    "        for trap in self.trap_states:\n",
    "            grid[trap] = -1\n",
    "        grid[self.goal_state] = 1\n",
    "        \n",
    "        if state:\n",
    "            grid[state] = 0.5\n",
    "        \n",
    "        if values is not None:\n",
    "            # Show value function\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "            \n",
    "            # Grid visualization\n",
    "            im1 = ax1.imshow(grid, cmap='RdYlGn', vmin=-1, vmax=1)\n",
    "            ax1.set_title('Grid World', fontsize=12)\n",
    "            ax1.set_xticks(range(self.size))\n",
    "            ax1.set_yticks(range(self.size))\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add labels\n",
    "            for i in range(self.size):\n",
    "                for j in range(self.size):\n",
    "                    if (i, j) == self.goal_state:\n",
    "                        ax1.text(j, i, 'G', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "                    elif (i, j) in self.trap_states:\n",
    "                        ax1.text(j, i, 'T', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "            \n",
    "            # Value function visualization\n",
    "            value_grid = values.reshape(self.size, self.size)\n",
    "            im2 = ax2.imshow(value_grid, cmap='coolwarm')\n",
    "            ax2.set_title('Value Function', fontsize=12)\n",
    "            ax2.set_xticks(range(self.size))\n",
    "            ax2.set_yticks(range(self.size))\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add value labels\n",
    "            for i in range(self.size):\n",
    "                for j in range(self.size):\n",
    "                    ax2.text(j, i, f'{value_grid[i,j]:.2f}', \n",
    "                            ha='center', va='center', fontsize=9)\n",
    "            \n",
    "            plt.colorbar(im2, ax=ax2)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            plt.imshow(grid, cmap='RdYlGn', vmin=-1, vmax=1)\n",
    "            plt.title('Grid World MDP', fontsize=14)\n",
    "            plt.colorbar()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "# Create and visualize the MDP\n",
    "mdp = GridWorldMDP(size=5)\n",
    "mdp.render()\n",
    "\n",
    "print(\"MDP Properties:\")\n",
    "print(f\"- State space size: {mdp.n_states}\")\n",
    "print(f\"- Action space size: {mdp.n_actions}\")\n",
    "print(f\"- Goal state: {mdp.goal_state}\")\n",
    "print(f\"- Trap states: {mdp.trap_states}\")\n",
    "print(\"\\nG = Goal (reward: +10)\")\n",
    "print(\"T = Trap (reward: -5)\")\n",
    "print(\"Other transitions: -0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='value-policy'></a>4. Value Functions vs Policies\n",
    "\n",
    "### Policy (π)\n",
    "A **policy** π(a|s) defines the agent's behavior by specifying the probability of taking action a in state s.\n",
    "\n",
    "### Value Function (V)\n",
    "The **state value function** V^π(s) represents the expected return when starting from state s and following policy π:\n",
    "\n",
    "$$V^\\pi(s) = E_\\pi[\\sum_{t=0}^{\\infty} \\gamma^t R_{t+1} | S_0 = s]$$\n",
    "\n",
    "### Action-Value Function (Q)\n",
    "The **action-value function** Q^π(s,a) represents the expected return when starting from state s, taking action a, and then following policy π:\n",
    "\n",
    "$$Q^\\pi(s,a) = E_\\pi[\\sum_{t=0}^{\\infty} \\gamma^t R_{t+1} | S_0 = s, A_0 = a]$$\n",
    "\n",
    "Let's implement value iteration and policy iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIteration:\n",
    "    \"\"\"Value iteration algorithm for solving MDPs.\"\"\"\n",
    "    \n",
    "    def __init__(self, mdp, gamma=0.9, theta=1e-6):\n",
    "        self.mdp = mdp\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        self.values = np.zeros(mdp.n_states)\n",
    "        self.policy = np.zeros((mdp.n_states, mdp.n_actions))\n",
    "    \n",
    "    def state_to_index(self, state):\n",
    "        \"\"\"Convert 2D state to 1D index.\"\"\"\n",
    "        return state[0] * self.mdp.size + state[1]\n",
    "    \n",
    "    def index_to_state(self, index):\n",
    "        \"\"\"Convert 1D index to 2D state.\"\"\"\n",
    "        return (index // self.mdp.size, index % self.mdp.size)\n",
    "    \n",
    "    def solve(self, max_iterations=1000):\n",
    "        \"\"\"Run value iteration algorithm.\"\"\"\n",
    "        iteration_values = []  # Store values for visualization\n",
    "        \n",
    "        for iteration in range(max_iterations):\n",
    "            delta = 0\n",
    "            new_values = np.copy(self.values)\n",
    "            \n",
    "            for state_idx in range(self.mdp.n_states):\n",
    "                state = self.index_to_state(state_idx)\n",
    "                \n",
    "                if state == self.mdp.goal_state:\n",
    "                    continue\n",
    "                \n",
    "                action_values = []\n",
    "                for action in range(self.mdp.n_actions):\n",
    "                    next_state, reward, _ = self.mdp.step(state, action)\n",
    "                    next_idx = self.state_to_index(next_state)\n",
    "                    action_values.append(reward + self.gamma * self.values[next_idx])\n",
    "                \n",
    "                new_values[state_idx] = max(action_values)\n",
    "                delta = max(delta, abs(new_values[state_idx] - self.values[state_idx]))\n",
    "            \n",
    "            self.values = new_values\n",
    "            iteration_values.append(np.copy(self.values))\n",
    "            \n",
    "            if delta < self.theta:\n",
    "                print(f\"Value iteration converged in {iteration + 1} iterations\")\n",
    "                break\n",
    "        \n",
    "        # Extract policy from values\n",
    "        self.extract_policy()\n",
    "        return iteration_values\n",
    "    \n",
    "    def extract_policy(self):\n",
    "        \"\"\"Extract greedy policy from value function.\"\"\"\n",
    "        for state_idx in range(self.mdp.n_states):\n",
    "            state = self.index_to_state(state_idx)\n",
    "            \n",
    "            if state == self.mdp.goal_state:\n",
    "                continue\n",
    "            \n",
    "            action_values = []\n",
    "            for action in range(self.mdp.n_actions):\n",
    "                next_state, reward, _ = self.mdp.step(state, action)\n",
    "                next_idx = self.state_to_index(next_state)\n",
    "                action_values.append(reward + self.gamma * self.values[next_idx])\n",
    "            \n",
    "            best_action = np.argmax(action_values)\n",
    "            self.policy[state_idx] = np.zeros(self.mdp.n_actions)\n",
    "            self.policy[state_idx, best_action] = 1.0\n",
    "    \n",
    "    def visualize_policy(self):\n",
    "        \"\"\"Visualize the learned policy.\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        \n",
    "        # Create grid for visualization\n",
    "        grid = np.zeros((self.mdp.size, self.mdp.size))\n",
    "        \n",
    "        # Mark special states\n",
    "        for trap in self.mdp.trap_states:\n",
    "            grid[trap] = -0.5\n",
    "        grid[self.mdp.goal_state] = 1\n",
    "        \n",
    "        ax.imshow(grid, cmap='RdYlGn', alpha=0.3, vmin=-1, vmax=1)\n",
    "        \n",
    "        # Draw policy arrows\n",
    "        arrow_scale = 0.3\n",
    "        for i in range(self.mdp.size):\n",
    "            for j in range(self.mdp.size):\n",
    "                state = (i, j)\n",
    "                state_idx = self.state_to_index(state)\n",
    "                \n",
    "                if state == self.mdp.goal_state:\n",
    "                    ax.text(j, i, 'GOAL', ha='center', va='center', \n",
    "                           fontsize=10, fontweight='bold', color='green')\n",
    "                    continue\n",
    "                \n",
    "                if state in self.mdp.trap_states:\n",
    "                    ax.text(j, i, 'TRAP', ha='center', va='center', \n",
    "                           fontsize=10, fontweight='bold', color='red')\n",
    "                \n",
    "                # Get best action\n",
    "                best_action = np.argmax(self.policy[state_idx])\n",
    "                \n",
    "                # Draw arrow\n",
    "                if best_action == 0:  # Up\n",
    "                    ax.arrow(j, i, 0, -arrow_scale, head_width=0.1, \n",
    "                            head_length=0.1, fc='blue', ec='blue')\n",
    "                elif best_action == 1:  # Down\n",
    "                    ax.arrow(j, i, 0, arrow_scale, head_width=0.1, \n",
    "                            head_length=0.1, fc='blue', ec='blue')\n",
    "                elif best_action == 2:  # Left\n",
    "                    ax.arrow(j, i, -arrow_scale, 0, head_width=0.1, \n",
    "                            head_length=0.1, fc='blue', ec='blue')\n",
    "                elif best_action == 3:  # Right\n",
    "                    ax.arrow(j, i, arrow_scale, 0, head_width=0.1, \n",
    "                            head_length=0.1, fc='blue', ec='blue')\n",
    "        \n",
    "        ax.set_title('Learned Policy (Arrows show best action)', fontsize=14)\n",
    "        ax.set_xticks(range(self.mdp.size))\n",
    "        ax.set_yticks(range(self.mdp.size))\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xlabel('Column', fontsize=12)\n",
    "        ax.set_ylabel('Row', fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Solve the MDP using value iteration\n",
    "vi_solver = ValueIteration(mdp, gamma=0.9)\n",
    "iteration_values = vi_solver.solve()\n",
    "\n",
    "# Visualize the value function\n",
    "mdp.render(values=vi_solver.values)\n",
    "\n",
    "# Visualize the policy\n",
    "vi_solver.visualize_policy()\n",
    "\n",
    "# Plot convergence\n",
    "plt.figure(figsize=(10, 6))\n",
    "convergence_data = [np.max(np.abs(iteration_values[i] - iteration_values[-1])) \n",
    "                   for i in range(len(iteration_values))]\n",
    "plt.semilogy(convergence_data, linewidth=2)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Max Value Difference from Converged (log scale)', fontsize=12)\n",
    "plt.title('Value Iteration Convergence', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='approximation'></a>5. Value Function Approximation\n",
    "\n",
    "For large or continuous state spaces, we can't store values for every state. Instead, we use **function approximation**:\n",
    "\n",
    "$$V(s) \\approx \\hat{V}(s, \\theta)$$\n",
    "\n",
    "Where θ are the parameters of our approximator (e.g., neural network weights).\n",
    "\n",
    "Common approaches:\n",
    "- **Linear approximation**: $\\hat{V}(s) = \\theta^T \\phi(s)$\n",
    "- **Neural networks**: Deep Q-Networks (DQN)\n",
    "- **Tile coding**: Discretization of continuous spaces\n",
    "\n",
    "Let's demonstrate linear function approximation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearFunctionApproximator:\n",
    "    \"\"\"Linear function approximation for value functions.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_features, learning_rate=0.01):\n",
    "        self.weights = np.random.randn(n_features) * 0.01\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def feature_extraction(self, state):\n",
    "        \"\"\"Extract features from state (customizable).\"\"\"\n",
    "        # Example: polynomial features for 2D state\n",
    "        x, y = state\n",
    "        features = np.array([\n",
    "            1,  # Bias\n",
    "            x, y,  # Linear terms\n",
    "            x**2, y**2,  # Quadratic terms\n",
    "            x*y,  # Interaction term\n",
    "            np.exp(-(x**2 + y**2)/10)  # RBF feature\n",
    "        ])\n",
    "        return features\n",
    "    \n",
    "    def predict(self, state):\n",
    "        \"\"\"Predict value for a state.\"\"\"\n",
    "        features = self.feature_extraction(state)\n",
    "        return np.dot(self.weights, features)\n",
    "    \n",
    "    def update(self, state, target):\n",
    "        \"\"\"Update weights using gradient descent.\"\"\"\n",
    "        features = self.feature_extraction(state)\n",
    "        prediction = self.predict(state)\n",
    "        error = target - prediction\n",
    "        \n",
    "        # Gradient descent update\n",
    "        self.weights += self.learning_rate * error * features\n",
    "        \n",
    "        return error\n",
    "\n",
    "\n",
    "# Demonstrate function approximation\n",
    "def demonstrate_approximation():\n",
    "    \"\"\"Show how function approximation learns to represent value function.\"\"\"\n",
    "    \n",
    "    # Create a simple 2D value function to approximate\n",
    "    def true_value_function(x, y):\n",
    "        return 10 * np.exp(-((x-2)**2 + (y-2)**2)/4) - 5 * np.exp(-((x-3)**2 + (y-1)**2)/2)\n",
    "    \n",
    "    # Generate training data\n",
    "    n_samples = 500\n",
    "    X = np.random.uniform(0, 5, (n_samples, 2))\n",
    "    y = [true_value_function(x[0], x[1]) for x in X]\n",
    "    \n",
    "    # Train approximator\n",
    "    approximator = LinearFunctionApproximator(n_features=7, learning_rate=0.01)\n",
    "    \n",
    "    errors = []\n",
    "    for epoch in range(100):\n",
    "        epoch_error = 0\n",
    "        for i in range(n_samples):\n",
    "            error = approximator.update(X[i], y[i])\n",
    "            epoch_error += error**2\n",
    "        errors.append(epoch_error / n_samples)\n",
    "    \n",
    "    # Visualize results\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: True value function\n",
    "    ax1 = fig.add_subplot(131, projection='3d')\n",
    "    x_grid = np.linspace(0, 5, 30)\n",
    "    y_grid = np.linspace(0, 5, 30)\n",
    "    X_grid, Y_grid = np.meshgrid(x_grid, y_grid)\n",
    "    Z_true = np.array([[true_value_function(x, y) for x in x_grid] for y in y_grid])\n",
    "    \n",
    "    ax1.plot_surface(X_grid, Y_grid, Z_true, cmap='viridis', alpha=0.8)\n",
    "    ax1.set_title('True Value Function', fontsize=12)\n",
    "    ax1.set_xlabel('X')\n",
    "    ax1.set_ylabel('Y')\n",
    "    ax1.set_zlabel('Value')\n",
    "    \n",
    "    # Plot 2: Approximated value function\n",
    "    ax2 = fig.add_subplot(132, projection='3d')\n",
    "    Z_approx = np.array([[approximator.predict([x, y]) for x in x_grid] for y in y_grid])\n",
    "    \n",
    "    ax2.plot_surface(X_grid, Y_grid, Z_approx, cmap='viridis', alpha=0.8)\n",
    "    ax2.set_title('Approximated Value Function', fontsize=12)\n",
    "    ax2.set_xlabel('X')\n",
    "    ax2.set_ylabel('Y')\n",
    "    ax2.set_zlabel('Value')\n",
    "    \n",
    "    # Plot 3: Training error\n",
    "    ax3 = fig.add_subplot(133)\n",
    "    ax3.plot(errors, linewidth=2)\n",
    "    ax3.set_xlabel('Epoch', fontsize=12)\n",
    "    ax3.set_ylabel('Mean Squared Error', fontsize=12)\n",
    "    ax3.set_title('Training Error', fontsize=12)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Final MSE: {errors[-1]:.4f}\")\n",
    "    print(f\"Feature weights: {approximator.weights}\")\n",
    "\n",
    "\n",
    "demonstrate_approximation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='dqn'></a>6. Deep Q-Network (DQN) for MountainCar\n",
    "\n",
    "Now let's implement a complete DQN agent to solve the MountainCar-v0 environment. DQN combines:\n",
    "- Neural network for Q-function approximation\n",
    "- Experience replay for stable learning\n",
    "- Target network for stability\n",
    "- ε-greedy exploration\n",
    "\n",
    "### MountainCar Environment\n",
    "- **Goal**: Drive a car up a mountain\n",
    "- **Challenge**: Car doesn't have enough power to go straight up\n",
    "- **Strategy**: Must build momentum by going back and forth\n",
    "- **State**: [position, velocity]\n",
    "- **Actions**: [push left, no push, push right]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience replay buffer\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition.\"\"\"\n",
    "        self.buffer.append(Transition(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of transitions.\"\"\"\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# DQN Network\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"Deep Q-Network architecture.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            nn.init.constant_(module.bias, 0.01)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# DQN Agent\n",
    "class DQNAgent:\n",
    "    \"\"\"DQN agent for MountainCar.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, \n",
    "                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995,\n",
    "                 buffer_size=10000, batch_size=64, tau=0.001):\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "        \n",
    "        # Neural networks\n",
    "        self.q_network = DQN(state_dim, action_dim).to(device)\n",
    "        self.target_network = DQN(state_dim, action_dim).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.memory = ReplayBuffer(buffer_size)\n",
    "        \n",
    "        # Tracking\n",
    "        self.losses = []\n",
    "        self.rewards = []\n",
    "        self.epsilons = []\n",
    "    \n",
    "    def select_action(self, state, training=True):\n",
    "        \"\"\"Select action using epsilon-greedy policy.\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            q_values = self.q_network(state_tensor)\n",
    "            return q_values.argmax(1).item()\n",
    "    \n",
    "    def store_transition(self, state, action, next_state, reward, done):\n",
    "        \"\"\"Store transition in replay buffer.\"\"\"\n",
    "        self.memory.push(state, action, next_state, reward, done)\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Update Q-network using experience replay.\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sample batch\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        # Convert to tensors\n",
    "        state_batch = torch.FloatTensor(batch.state).to(device)\n",
    "        action_batch = torch.LongTensor(batch.action).to(device)\n",
    "        reward_batch = torch.FloatTensor(batch.reward).to(device)\n",
    "        next_state_batch = torch.FloatTensor(batch.next_state).to(device)\n",
    "        done_batch = torch.FloatTensor(batch.done).to(device)\n",
    "        \n",
    "        # Current Q values\n",
    "        current_q_values = self.q_network(state_batch).gather(1, action_batch.unsqueeze(1))\n",
    "        \n",
    "        # Next Q values from target network\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_state_batch).max(1)[0]\n",
    "            target_q_values = reward_batch + (1 - done_batch) * self.gamma * next_q_values\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Soft update target network\n",
    "        self.soft_update_target_network()\n",
    "        \n",
    "        self.losses.append(loss.item())\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def soft_update_target_network(self):\n",
    "        \"\"\"Soft update of target network parameters.\"\"\"\n",
    "        for target_param, param in zip(self.target_network.parameters(), \n",
    "                                       self.q_network.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + \n",
    "                                   (1.0 - self.tau) * target_param.data)\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay exploration rate.\"\"\"\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "        self.epsilons.append(self.epsilon)\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_dqn_mountaincar(n_episodes=500, max_steps=200, render_interval=100):\n",
    "    \"\"\"Train DQN on MountainCar-v0.\"\"\"\n",
    "    \n",
    "    # Create environment\n",
    "    env = gym.make('MountainCar-v0')\n",
    "    \n",
    "    # Get dimensions\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    print(f\"State dimension: {state_dim}\")\n",
    "    print(f\"Action dimension: {action_dim}\")\n",
    "    print(f\"Actions: 0=Push left, 1=No push, 2=Push right\\n\")\n",
    "    \n",
    "    # Create agent\n",
    "    agent = DQNAgent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        lr=0.001,\n",
    "        gamma=0.99,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.01,\n",
    "        epsilon_decay=0.995,\n",
    "        buffer_size=10000,\n",
    "        batch_size=64,\n",
    "        tau=0.001\n",
    "    )\n",
    "    \n",
    "    # Training metrics\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    moving_avg_rewards = []\n",
    "    \n",
    "    # Training loop\n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "        \n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Select and execute action\n",
    "            action = agent.select_action(state)\n",
    "            result = env.step(action)\n",
    "            \n",
    "            # Handle different gym versions\n",
    "            if len(result) == 5:\n",
    "                next_state, reward, terminated, truncated, _ = result\n",
    "                done = terminated or truncated\n",
    "            else:\n",
    "                next_state, reward, done, _ = result\n",
    "            \n",
    "            # Custom reward shaping for MountainCar\n",
    "            position, velocity = next_state\n",
    "            \n",
    "            # Reward shaping: encourage reaching the goal\n",
    "            shaped_reward = reward\n",
    "            if position >= 0.5:  # Reached goal\n",
    "                shaped_reward = 10.0\n",
    "            else:\n",
    "                # Small penalty for time and bonus for height/velocity\n",
    "                shaped_reward = -0.1 + position * 0.1 + abs(velocity) * 0.1\n",
    "            \n",
    "            # Store transition\n",
    "            agent.store_transition(state, action, next_state, shaped_reward, done)\n",
    "            \n",
    "            # Update network\n",
    "            loss = agent.update()\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Decay epsilon\n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        # Record metrics\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(step + 1)\n",
    "        \n",
    "        # Calculate moving average\n",
    "        if len(episode_rewards) >= 100:\n",
    "            moving_avg = np.mean(episode_rewards[-100:])\n",
    "        else:\n",
    "            moving_avg = np.mean(episode_rewards)\n",
    "        moving_avg_rewards.append(moving_avg)\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            print(f\"Episode {episode + 1}/{n_episodes} | \"\n",
    "                  f\"Reward: {total_reward:.2f} | \"\n",
    "                  f\"Steps: {step + 1} | \"\n",
    "                  f\"Epsilon: {agent.epsilon:.3f} | \"\n",
    "                  f\"Avg Reward (100 ep): {moving_avg:.2f}\")\n",
    "        \n",
    "        # Render occasionally\n",
    "        if (episode + 1) % render_interval == 0 and episode > 0:\n",
    "            print(f\"\\n--- Rendering episode {episode + 1} ---\")\n",
    "            test_state = env.reset()\n",
    "            if isinstance(test_state, tuple):\n",
    "                test_state = test_state[0]\n",
    "            \n",
    "            for _ in range(max_steps):\n",
    "                env.render()\n",
    "                action = agent.select_action(test_state, training=False)\n",
    "                result = env.step(action)\n",
    "                \n",
    "                if len(result) == 5:\n",
    "                    test_state, _, terminated, truncated, _ = result\n",
    "                    done = terminated or truncated\n",
    "                else:\n",
    "                    test_state, _, done, _ = result\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    return agent, episode_rewards, episode_lengths, moving_avg_rewards\n",
    "\n",
    "\n",
    "# Train the agent\n",
    "print(\"Training DQN on MountainCar-v0...\\n\")\n",
    "agent, rewards, lengths, moving_avg = train_dqn_mountaincar(n_episodes=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='visualization'></a>7. Training Visualization and Analysis\n",
    "\n",
    "Let's visualize the training process and analyze the agent's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Episode Rewards\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "ax1.plot(rewards, alpha=0.3, color='blue', label='Episode Reward')\n",
    "ax1.plot(moving_avg, color='red', linewidth=2, label='Moving Average (100 episodes)')\n",
    "ax1.set_xlabel('Episode', fontsize=11)\n",
    "ax1.set_ylabel('Total Reward', fontsize=11)\n",
    "ax1.set_title('Training Progress: Episode Rewards', fontsize=13)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Episode Lengths\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "ax2.plot(lengths, alpha=0.5, color='green')\n",
    "ax2.set_xlabel('Episode', fontsize=11)\n",
    "ax2.set_ylabel('Episode Length', fontsize=11)\n",
    "ax2.set_title('Steps per Episode', fontsize=12)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Loss curve\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "if len(agent.losses) > 0:\n",
    "    # Smooth the loss curve\n",
    "    window_size = min(100, len(agent.losses) // 10)\n",
    "    if window_size > 0:\n",
    "        smoothed_losses = np.convolve(agent.losses, \n",
    "                                      np.ones(window_size)/window_size, \n",
    "                                      mode='valid')\n",
    "        ax3.plot(smoothed_losses, color='purple', alpha=0.7)\n",
    "    ax3.set_xlabel('Update Step', fontsize=11)\n",
    "    ax3.set_ylabel('Loss', fontsize=11)\n",
    "    ax3.set_title('Training Loss (MSE)', fontsize=12)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Epsilon decay\n",
    "ax4 = fig.add_subplot(gs[1, 2])\n",
    "ax4.plot(agent.epsilons, color='orange', linewidth=2)\n",
    "ax4.set_xlabel('Episode', fontsize=11)\n",
    "ax4.set_ylabel('Epsilon', fontsize=11)\n",
    "ax4.set_title('Exploration Rate Decay', fontsize=12)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Reward distribution\n",
    "ax5 = fig.add_subplot(gs[2, 0])\n",
    "ax5.hist(rewards, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "ax5.set_xlabel('Total Reward', fontsize=11)\n",
    "ax5.set_ylabel('Frequency', fontsize=11)\n",
    "ax5.set_title('Reward Distribution', fontsize=12)\n",
    "ax5.axvline(np.mean(rewards), color='red', linestyle='--', \n",
    "           label=f'Mean: {np.mean(rewards):.2f}')\n",
    "ax5.legend(fontsize=10)\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Success rate over time\n",
    "ax6 = fig.add_subplot(gs[2, 1])\n",
    "window = 50\n",
    "success_threshold = -110  # MountainCar success if reward > -110\n",
    "success_rate = []\n",
    "for i in range(len(rewards)):\n",
    "    start = max(0, i - window + 1)\n",
    "    window_rewards = rewards[start:i+1]\n",
    "    success_rate.append(sum(r > success_threshold for r in window_rewards) / len(window_rewards) * 100)\n",
    "\n",
    "ax6.plot(success_rate, color='darkgreen', linewidth=2)\n",
    "ax6.set_xlabel('Episode', fontsize=11)\n",
    "ax6.set_ylabel('Success Rate (%)', fontsize=11)\n",
    "ax6.set_title(f'Success Rate (Rolling {window} episodes)', fontsize=12)\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Q-value statistics\n",
    "ax7 = fig.add_subplot(gs[2, 2])\n",
    "# Sample some states to check Q-values\n",
    "test_states = [\n",
    "    [-1.2, 0.0],  # Starting position\n",
    "    [-0.5, 0.03],  # Mid position\n",
    "    [0.3, 0.04],   # Near goal\n",
    "    [0.5, 0.0]     # Goal position\n",
    "]\n",
    "q_values_test = []\n",
    "with torch.no_grad():\n",
    "    for state in test_states:\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        q_vals = agent.q_network(state_tensor).cpu().numpy()[0]\n",
    "        q_values_test.append(q_vals)\n",
    "\n",
    "positions = ['Start', 'Mid', 'Near Goal', 'Goal']\n",
    "x = np.arange(len(positions))\n",
    "width = 0.25\n",
    "\n",
    "for i in range(3):\n",
    "    values = [q[i] for q in q_values_test]\n",
    "    ax7.bar(x + i*width, values, width, label=f'Action {i}')\n",
    "\n",
    "ax7.set_xlabel('Position', fontsize=11)\n",
    "ax7.set_ylabel('Q-value', fontsize=11)\n",
    "ax7.set_title('Learned Q-values at Different Positions', fontsize=12)\n",
    "ax7.set_xticks(x + width)\n",
    "ax7.set_xticklabels(positions)\n",
    "ax7.legend(fontsize=9)\n",
    "ax7.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('DQN Training Analysis - MountainCar-v0', fontsize=15, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Episodes: {len(rewards)}\")\n",
    "print(f\"Final Epsilon: {agent.epsilon:.4f}\")\n",
    "print(f\"\\nReward Statistics:\")\n",
    "print(f\"  - Mean Reward: {np.mean(rewards):.2f}\")\n",
    "print(f\"  - Std Reward: {np.std(rewards):.2f}\")\n",
    "print(f\"  - Min Reward: {np.min(rewards):.2f}\")\n",
    "print(f\"  - Max Reward: {np.max(rewards):.2f}\")\n",
    "print(f\"  - Final 100 Episodes Mean: {np.mean(rewards[-100:]):.2f}\")\n",
    "print(f\"\\nEpisode Length Statistics:\")\n",
    "print(f\"  - Mean Length: {np.mean(lengths):.2f}\")\n",
    "print(f\"  - Min Length: {np.min(lengths)}\")\n",
    "print(f\"  - Max Length: {np.max(lengths)}\")\n",
    "print(f\"\\nSuccess Rate (last 100 episodes): {success_rate[-1]:.1f}%\")\n",
    "print(f\"Total Network Updates: {len(agent.losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Trained Agent\n",
    "\n",
    "Let's test our trained agent and visualize its learned policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(agent, n_episodes=5, render=False):\n",
    "    \"\"\"Test the trained agent.\"\"\"\n",
    "    env = gym.make('MountainCar-v0')\n",
    "    \n",
    "    test_rewards = []\n",
    "    test_trajectories = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "        \n",
    "        trajectory = []\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(200):\n",
    "            if render:\n",
    "                env.render()\n",
    "            \n",
    "            trajectory.append(state.copy())\n",
    "            \n",
    "            # Use greedy policy (no exploration)\n",
    "            action = agent.select_action(state, training=False)\n",
    "            \n",
    "            result = env.step(action)\n",
    "            if len(result) == 5:\n",
    "                state, reward, terminated, truncated, _ = result\n",
    "                done = terminated or truncated\n",
    "            else:\n",
    "                state, reward, done, _ = result\n",
    "            \n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                trajectory.append(state.copy())\n",
    "                print(f\"Episode {episode + 1}: Reached goal in {step + 1} steps! Total reward: {total_reward:.2f}\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Episode {episode + 1}: Did not reach goal. Total reward: {total_reward:.2f}\")\n",
    "        \n",
    "        test_rewards.append(total_reward)\n",
    "        test_trajectories.append(np.array(trajectory))\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    return test_rewards, test_trajectories\n",
    "\n",
    "\n",
    "# Test the agent\n",
    "print(\"Testing trained agent...\\n\")\n",
    "test_rewards, test_trajectories = test_agent(agent, n_episodes=5)\n",
    "\n",
    "# Visualize test trajectories\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, traj in enumerate(test_trajectories[:6]):\n",
    "    if i < len(axes):\n",
    "        ax = axes[i]\n",
    "        positions = traj[:, 0]\n",
    "        velocities = traj[:, 1]\n",
    "        \n",
    "        # Color gradient for time\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, len(positions)))\n",
    "        \n",
    "        for j in range(len(positions) - 1):\n",
    "            ax.plot(positions[j:j+2], velocities[j:j+2], \n",
    "                   color=colors[j], linewidth=2)\n",
    "        \n",
    "        # Mark start and end\n",
    "        ax.scatter(positions[0], velocities[0], color='green', \n",
    "                  s=100, marker='o', label='Start', zorder=5)\n",
    "        ax.scatter(positions[-1], velocities[-1], color='red', \n",
    "                  s=100, marker='*', label='End', zorder=5)\n",
    "        \n",
    "        # Mark goal region\n",
    "        ax.axvline(x=0.5, color='red', linestyle='--', alpha=0.5, label='Goal')\n",
    "        \n",
    "        ax.set_xlabel('Position', fontsize=10)\n",
    "        ax.set_ylabel('Velocity', fontsize=10)\n",
    "        ax.set_title(f'Episode {i+1} Trajectory', fontsize=11)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.set_xlim([-1.3, 0.7])\n",
    "        ax.set_ylim([-0.08, 0.08])\n",
    "\n",
    "plt.suptitle('Agent Trajectories in State Space', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTest Performance:\")\n",
    "print(f\"Average test reward: {np.mean(test_rewards):.2f} ± {np.std(test_rewards):.2f}\")\n",
    "print(f\"Success rate: {sum(r > -200 for r in test_rewards) / len(test_rewards) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Learned Q-Function\n",
    "\n",
    "Let's create a heatmap visualization of the learned Q-values across the state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_q_function(agent, resolution=50):\n",
    "    \"\"\"Create heatmap of Q-values across state space.\"\"\"\n",
    "    \n",
    "    # Create grid of states\n",
    "    positions = np.linspace(-1.2, 0.6, resolution)\n",
    "    velocities = np.linspace(-0.07, 0.07, resolution)\n",
    "    \n",
    "    # Compute Q-values for each state-action pair\n",
    "    q_values_grid = np.zeros((3, resolution, resolution))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, pos in enumerate(positions):\n",
    "            for j, vel in enumerate(velocities):\n",
    "                state = torch.FloatTensor([pos, vel]).unsqueeze(0).to(device)\n",
    "                q_vals = agent.q_network(state).cpu().numpy()[0]\n",
    "                q_values_grid[:, j, i] = q_vals\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "    \n",
    "    action_names = ['Push Left', 'No Push', 'Push Right']\n",
    "    \n",
    "    # Plot Q-values for each action\n",
    "    for idx, (ax, action_name) in enumerate(zip(axes[:3], action_names)):\n",
    "        im = ax.imshow(q_values_grid[idx], aspect='auto', origin='lower',\n",
    "                      extent=[-1.2, 0.6, -0.07, 0.07], cmap='coolwarm')\n",
    "        ax.set_xlabel('Position', fontsize=11)\n",
    "        ax.set_ylabel('Velocity', fontsize=11)\n",
    "        ax.set_title(f'Q-values for {action_name}', fontsize=12)\n",
    "        ax.axvline(x=0.5, color='green', linestyle='--', alpha=0.7, linewidth=2)\n",
    "        plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Plot best action\n",
    "    best_actions = np.argmax(q_values_grid, axis=0)\n",
    "    im = axes[3].imshow(best_actions, aspect='auto', origin='lower',\n",
    "                       extent=[-1.2, 0.6, -0.07, 0.07], cmap='viridis',\n",
    "                       vmin=0, vmax=2)\n",
    "    axes[3].set_xlabel('Position', fontsize=11)\n",
    "    axes[3].set_ylabel('Velocity', fontsize=11)\n",
    "    axes[3].set_title('Best Action (Policy)', fontsize=12)\n",
    "    axes[3].axvline(x=0.5, color='white', linestyle='--', alpha=0.7, linewidth=2)\n",
    "    \n",
    "    # Create custom colorbar for actions\n",
    "    cbar = plt.colorbar(im, ax=axes[3], ticks=[0, 1, 2])\n",
    "    cbar.ax.set_yticklabels(['Left', 'None', 'Right'])\n",
    "    \n",
    "    plt.suptitle('Learned Q-Function and Policy Visualization', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_q_function(agent, resolution=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Trained Model\n",
    "\n",
    "Save the trained model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_path = 'dqn_mountaincar_model.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': agent.q_network.state_dict(),\n",
    "    'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "    'epsilon': agent.epsilon,\n",
    "    'episode_rewards': rewards,\n",
    "    'episode_lengths': lengths\n",
    "}, model_path)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")\n",
    "print(f\"File size: {os.path.getsize(model_path) / 1024:.2f} KB\")\n",
    "\n",
    "# Example of loading the model\n",
    "# checkpoint = torch.load(model_path)\n",
    "# agent.q_network.load_state_dict(checkpoint['model_state_dict'])\n",
    "# agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Further Exploration\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "1. **Exploration vs Exploitation**: We demonstrated how ε-greedy exploration balances discovering new strategies with exploiting known good actions.\n",
    "\n",
    "2. **MDPs**: We implemented a grid world to understand states, actions, transitions, and rewards.\n",
    "\n",
    "3. **Value Functions vs Policies**: We showed how value iteration can solve MDPs and extract optimal policies.\n",
    "\n",
    "4. **Function Approximation**: We used neural networks to approximate Q-values for continuous state spaces.\n",
    "\n",
    "5. **DQN Implementation**: We successfully trained a DQN agent to solve MountainCar using:\n",
    "   - Experience replay for stable learning\n",
    "   - Target networks for stability\n",
    "   - Reward shaping to guide learning\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "- The agent learns to build momentum by swinging back and forth\n",
    "- Q-values show clear action preferences in different state regions\n",
    "- Training is stable with proper hyperparameters\n",
    "- Success rate improves significantly over training\n",
    "\n",
    "### Ideas for Further Exploration\n",
    "\n",
    "1. **Algorithm Variations**:\n",
    "   - Implement Double DQN to reduce overestimation\n",
    "   - Try Dueling DQN architecture\n",
    "   - Implement Rainbow DQN combining multiple improvements\n",
    "\n",
    "2. **Different Environments**:\n",
    "   - CartPole-v1 (simpler, discrete actions)\n",
    "   - LunarLander-v2 (more complex dynamics)\n",
    "   - Atari games (high-dimensional observations)\n",
    "\n",
    "3. **Hyperparameter Tuning**:\n",
    "   - Learning rate schedules\n",
    "   - Different network architectures\n",
    "   - Replay buffer prioritization\n",
    "\n",
    "4. **Advanced Concepts**:\n",
    "   - Policy gradient methods (REINFORCE, A2C, PPO)\n",
    "   - Continuous action spaces with DDPG or SAC\n",
    "   - Model-based RL approaches\n",
    "\n",
    "### Resources for Further Learning\n",
    "\n",
    "- Sutton & Barto: \"Reinforcement Learning: An Introduction\"\n",
    "- OpenAI Spinning Up: Educational resource for deep RL\n",
    "- Stable Baselines3: Production-ready RL algorithms\n",
    "- DeepMind's RL Course: Comprehensive video lectures\n",
    "\n",
    "This notebook provides a foundation for understanding and implementing reinforcement learning algorithms. The concepts and code can be extended to tackle more complex problems in robotics, game playing, resource optimization, and many other domains."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
